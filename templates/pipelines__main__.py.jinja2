import json
import zipfile
from pathlib import Path
from {{ project_name }}.logger import logger
from sqlalchemy import text
from {{ project_name }}.src.db.database import run_with_session
from {{ project_name }}.src.db.system_models import PipelineProgress
{% for pipeline_name in pipeline_names %}
from .{{ pipeline_name }}.__main__ import run_all as run_{{ pipeline_name }}
{% endfor %}

def ensure_zips_extracted():
    """Extract ZIP files if needed based on manifest"""
    manifest_path = Path(__file__).parent.parent.parent / "extraction_manifest.json"
    
    if not manifest_path.exists():
        logger.error("âš ï¸  No extraction manifest found - ZIPs may not be extracted")
        return
    
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)
    
    logger.info("ðŸ“¦ Checking ZIP extractions...")
    
    for extraction in manifest["extractions"]:
        zip_path = Path(extraction["zip_path"])
        extract_dir = Path(extraction["extract_dir"])
        
        # Check if extraction is needed
        if not extract_dir.exists() or not _extraction_is_current(zip_path, extract_dir):
            logger.info(f"ðŸ“‚ Extracting {zip_path.name}...")
            extract_dir.mkdir(exist_ok=True)
            
            with zipfile.ZipFile(zip_path, 'r') as zf:
                zf.extractall(extract_dir)
        else:
            logger.info(f"âœ… {zip_path.name} already extracted")

def _extraction_is_current(zip_path: Path, extract_dir: Path) -> bool:
    """Check if extraction is up to date"""
    if not extract_dir.exists():
        return False
    
    # Simple check: if extract dir is newer than ZIP file
    try:
        zip_mtime = zip_path.stat().st_mtime
        extract_mtime = extract_dir.stat().st_mtime
        return extract_mtime >= zip_mtime
    except (OSError, FileNotFoundError):
        return False

def get_pipeline_status(db) -> dict:
    """Get status of all pipelines from pipeline_progress table"""
    progress_records = db.query(PipelineProgress).all()
    return {p.table_name: p for p in progress_records}

def run_all_pipelines(db):
    ensure_zips_extracted()
    print("ðŸš€ Starting all data pipelines...")
    
    # Get pipeline status
    pipeline_status = get_pipeline_status(db)
    
    # Pipeline mapping
    pipeline_runners = {
        {% for pipeline_name in pipeline_names %}
        "{{ pipeline_name }}": run_{{ pipeline_name }},
        {% endfor %}
    }
    
    completed_count = 0
    in_progress_count = 0
    to_run_count = 0
    
    # Check each pipeline
    for pipeline_name, runner in pipeline_runners.items():
        progress = pipeline_status.get(pipeline_name)

        logger.info(f"Pipeline Progress: {progress}")
        
        if progress and progress.status == "completed":
            print(f"âœ… Skipping {pipeline_name} - already completed ({progress.total_rows:,} rows)")
            completed_count += 1
        elif progress and progress.status == "in_progress":
            print(f"ðŸ”„ Resuming {pipeline_name} from row {progress.last_row_processed:,}/{progress.total_rows:,}")
            runner(db)
            in_progress_count += 1
        else:
            print(f"ðŸ†• Starting {pipeline_name}")
            runner(db)
            to_run_count += 1
    
    print(f"\nâœ… Pipeline execution complete!")
    print(f"   Skipped (completed): {completed_count}")
    print(f"   Resumed: {in_progress_count}")
    print(f"   Started fresh: {to_run_count}")

if __name__ == "__main__":
    run_with_session(run_all_pipelines)